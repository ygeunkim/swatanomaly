---
bibliography: swatanomalyref.bib
output: 
  github_document:
    pandoc_args: --webtex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  comment = "#>",
  collapse = TRUE,
  out.width = "70%",
  fig.align = "center",
  fig.width = 6,
  fig.asp = .618
  )
options(digits = 3)
pander::panderOptions("round", 3)
```

# `swatanomaly`

Various anomaly detection algorithms studied in *NSR anomaly detection project*

- NND (baseline-method): @Yun:2018di
- K-L divergence: @Cho:2019ji

## Installation

```{r, eval=FALSE}
devtools::install_github("ygeunkim/swatanomaly")
```

```{r}
library(swatanomaly)
```

<!-- ## Nearest-Neighbor Distance Algorithm -->

<!-- This algorithm follows k-fold cross-validation construction. -->

<!-- ### Windowed NNS -->

<!-- |123456789|123456789|123456789|123456789|123456789|   -->
<!-- |:-------:|:-------:|:-------:|:-------:|:-------:|   -->
<!-- |ooooooooo|ooooooooo|ooooooooo|ooooooooo|ooooooooo|   -->
<!-- |ooooooooo|ooooooooo|ooooooooo|ooooooooo|ooooooooo|   -->
<!-- |ooooooooo|ooooooooo|ooooooooo|ooooooooo|ooooooooo|   -->

<!-- As k-fold CV, one fold is validation block and the remainings are training. NND is computed versus training. Consider the first block. Then we should compute NND of 9 observations, respectively. -->

<!-- For each $i \leftarrow 1$ to w(size of the block), -->

<!-- 1. Compute **Euclidean distance** versus every point in the training block. -->
<!-- 2. Find minimum value of the distance value. -->
<!-- 3. It is the NND of this observation. -->

<!-- Repeat this process for every block. Here, Euclidean distance for multivariate series is computed by -->

<!-- $$d(\mathbf{x}, \mathbf{y}) = \sqrt{\sum_j^p (x_j - y_j)^2}$$ -->

<!-- See `euc_nnd()` and `nns_cpp()`. -->

<!-- ### Setting threshold -->

<!-- 1. arbitrary threshold -->
<!-- 2. right tail value of NND pdf -->

<!-- `euc_pdf()` produces this NND pdf. The procedure is similar to the above windowed NNS with smaller partition, but different input. -->

<!-- - `nns_cpp()` is provided window size (`win`), while -->
<!-- - `euc_pdf()` partition number (`partition`) -->

<!-- Using this pdf, we can compute its quantile or all at once using `nnd_thr()`. -->

<!-- ### Anomaly detection -->

<!-- Given the result of windowed NNS, compare with the threshold. Use `detect_nnd()`. It returns `TRUE` (anomaly) and `FALSE` (normal). Sometimes we want the other labels. In this case, `detect_nndvec` can give any label you want (`label`). For example, `-1` and `1`. -->


## Kullback-Leibler Divergence Algorithm

This algorithm implements neighboring-window method. Using `stats::density()`, we first estimate each probability mass. Then we can compute K-L divergence from $p$ to $q$ by

$$D_{KL} = \sum_{x \in \mathcal{X}} p(x) \ln \frac{p(x)}{q(x)}$$

where $\mathcal{X}$ is the support of $p$ ($f_1$).

The algorithm uses a threshold $\lambda$ and check if $D < \lambda$. If this holds, two windows can be said that they are derived from the same gaussian distribution.

### Fixed lambda algorithm

- Data: univariate series of size $n$
- Input: window size $win$, jump size for sliding window $jump$, threshold $\lambda$

Note that the number of window is

$$\frac{n - win}{jump} + 1 \equiv w$$


1. For $i \leftarrow 1$ to (w - 1) do
    1. if $D < \lambda$ then
        1. $f_1$ be the pdf of $i$-th window and $f_2$ be the pdf of (i + 1)-th window
        2. K-L divergence $d_i$ from $f_2$ to $f_1$
        3. Set $D = d_i$.
        4. $f^{\prime} = f_1$
    2. else
        1. $f^{\prime}$ be the pdf of normal and $f_2$ be the pdf of (i + 1)-th window
        2. K-L divergence $d_i$ from $f_2$ to $f^{\prime}$
        3. Set $D = d_i$.
2. output: $\{ d_1, \ldots, d_{w - 1} \}$


### Dynamic lambda algorithm

- Data: univariate series of size $n$
- Input: window size $win$, jump size for sliding window $jump$, threshold increment $\lambda^{\prime}$, threshold updating constant $\epsilon$

The threshold is initialized by

$$\lambda = \lambda^{\prime} \epsilon$$

If $D < \lambda$, it is updated by

$$\lambda = \lambda^{\prime} (d_{j - 2} + \epsilon)$$


1. $\lambda = \lambda^{\prime} \epsilon$
2. $d_1$ K-L from $1$-st to $2$-nd window
3. $d_2$ K-L from $2$-nd to $3$-rd window
4. For $i \leftarrow 3$ to (w - 1) do
    1. if $D < \lambda$ then
        1. $f_1$ be the pdf of $i$-th window and $f_2$ be the pdf of (i + 1)-th window
        2. K-L divergence $d_i$ from $f_2$ to $f_1$
        3. $\lambda = \lambda^{\prime} (d_{j - 2} + \epsilon)$
        4. Set $D = d_i$.
        5. $f^{\prime} = f_1$
    2. else
        1. $f^{\prime}$ be the pdf of normal and $f_2$ be the pdf of (i + 1)-th window
        2. K-L divergence $d_i$ from $f_2$ to $f^{\prime}$
        3. Set $D = d_i$.
5. output: $\{ d_1, \ldots, d_{w - 1} \}$


***

# References

